{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras import layers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import gc\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img, save_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data / agumment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our train data\n",
    "train_df = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting top 10 whales that are not new_whale, setting to new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_23a388d    73\n",
      "w_9b5109b    65\n",
      "w_9c506f6    62\n",
      "w_0369a5c    61\n",
      "w_700ebb4    57\n",
      "w_3de579a    54\n",
      "w_564a34b    51\n",
      "w_fd3e556    50\n",
      "w_88e4537    49\n",
      "w_2b069ba    48\n",
      "Name: Id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>004e8ad5b.jpg</td>\n",
       "      <td>w_3de579a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>00570db6b.jpg</td>\n",
       "      <td>w_9c506f6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image         Id\n",
       "26  004e8ad5b.jpg  w_3de579a\n",
       "32  00570db6b.jpg  w_9c506f6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collecting whales that are not new_whale\n",
    "whale_df = train_df[train_df.Id != 'new_whale']\n",
    "# getting top ten whales\n",
    "top_ten = whale_df[\"Id\"].value_counts().head(10)\n",
    "print(top_ten)\n",
    "\n",
    "# making a new df with top 10\n",
    "columns = ['Image', 'Id']\n",
    "new_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(len(train_df['Id'])):\n",
    "    if train_df['Id'].loc[i] in top_ten:\n",
    "        new_df.loc[i] = (train_df['Image'].loc[i], train_df['Id'].loc[i])\n",
    "new_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting df so we can have normal inputs for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  456\n",
      "Shape of X_test:  114\n",
      "Shape of y_train:  456\n",
      "Shape of y_test:  114\n",
      "25035    fc5c296f8.jpg\n",
      "19678    c669a3706.jpg\n",
      "Name: Image, dtype: object\n",
      "25035    w_0369a5c\n",
      "19678    w_700ebb4\n",
      "Name: Id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df['Image'], new_df['Id'], test_size=0.20)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape[0])\n",
    "print(\"Shape of X_test: \", X_test.shape[0])\n",
    "print(\"Shape of y_train: \", y_train.shape[0])\n",
    "print(\"Shape of y_test: \", y_test.shape[0])\n",
    "print(X_train[:2])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image data generator ( agumentation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.15,\n",
    "                        zoom_range=0.1, channel_shift_range=10,\n",
    "                        horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing every image from data set and resizing / agumentating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(data, dataset):\n",
    "    i = 0\n",
    "    for fig in data['Image']:\n",
    "        img = image.load_img(\"./input/\"+dataset+\"/\"+fig, target_size=(224, 224, 3))\n",
    "        img_arr = image.img_to_array(img)\n",
    "        image.save_img('./processed/224_224_0_'+fig, img_arr)\n",
    "        temp_image = np.expand_dims(mpimg.imread('./processed/224_224_0_'+fig), 0)\n",
    "        aug_iter = gen.flow(temp_image)\n",
    "        aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(8)]\n",
    "        for x in range(len(aug_images)):\n",
    "            image.save_img('./processed/224_224_'+str(x+1)+'_'+fig, aug_images[x])\n",
    "        if (i%100 == 0):\n",
    "            print(\"Processing image: \", i+1, \", \", fig)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processImages(new_df, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareImages(data, m, type_of_data):\n",
    "    \n",
    "    print(\"Preparing images\")\n",
    "    \n",
    "    if type_of_data == 'training':\n",
    "        X = np.zeros((m*6, 224, 224, 3))\n",
    "        count = 0\n",
    "        for fig in data:\n",
    "            #load images into images of size 100x100x3\n",
    "            for i in range(6):\n",
    "                img = image.load_img('./processed/224_224_' + str(i) + '_' + fig, target_size=(224, 224, 3))\n",
    "                x = image.img_to_array(img)\n",
    "                x = preprocess_input(x)\n",
    "                X[count] = x\n",
    "                count += 1\n",
    "                if (count%100 == 0):\n",
    "                    print(\"Processing image: \", count+1, \", \", fig)\n",
    "    elif type_of_data == 'testing':\n",
    "        X = np.zeros((m, 224, 224, 3))\n",
    "        count = 0\n",
    "        for fig in data:\n",
    "            img = image.load_img('./processed/224_224_0_' + fig, target_size=(224, 224, 3))\n",
    "            x = image.img_to_array(img)\n",
    "            x = preprocess_input(x)\n",
    "            X[count] = x\n",
    "            count += 1\n",
    "            if (count%50 == 0):\n",
    "                print(\"Processing image: \", count+1, \", \", fig)    \n",
    "    return X\n",
    "\n",
    "def prepare_labels(y):\n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    # print(integer_encoded)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    # print(onehot_encoded)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    # print(y.shape)\n",
    "    return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing images\n",
      "Processing image:  101 ,  36ef0f133.jpg\n",
      "Processing image:  201 ,  aa5dc4076.jpg\n",
      "Processing image:  301 ,  ba50a7085.jpg\n",
      "Processing image:  401 ,  9f6cc357a.jpg\n",
      "Processing image:  501 ,  258679fcf.jpg\n",
      "Processing image:  601 ,  e6bfde781.jpg\n",
      "Processing image:  701 ,  d4dd371d1.jpg\n",
      "Processing image:  801 ,  becb0e860.jpg\n",
      "Processing image:  901 ,  5336ac99b.jpg\n",
      "Processing image:  1001 ,  54f1d85a8.jpg\n",
      "Processing image:  1101 ,  c79ce1999.jpg\n",
      "Processing image:  1201 ,  317a126f4.jpg\n",
      "Processing image:  1301 ,  c4e291ada.jpg\n",
      "Processing image:  1401 ,  24100a9fc.jpg\n",
      "Processing image:  1501 ,  188b04c45.jpg\n",
      "Processing image:  1601 ,  b3a043eaf.jpg\n",
      "Processing image:  1701 ,  13a7495d5.jpg\n",
      "Processing image:  1801 ,  471e75940.jpg\n",
      "Processing image:  1901 ,  89afd1952.jpg\n",
      "Processing image:  2001 ,  462152c16.jpg\n",
      "Processing image:  2101 ,  a7e25cad5.jpg\n",
      "Processing image:  2201 ,  c5cc80b0c.jpg\n",
      "Processing image:  2301 ,  beb52d9f4.jpg\n",
      "Processing image:  2401 ,  1eef410a0.jpg\n",
      "Processing image:  2501 ,  82f7350f1.jpg\n",
      "Processing image:  2601 ,  9d55d3372.jpg\n",
      "Processing image:  2701 ,  2b6085d9c.jpg\n",
      "Preparing images\n",
      "Processing image:  51 ,  eaa3332d8.jpg\n",
      "Processing image:  101 ,  a6c4cab26.jpg\n"
     ]
    }
   ],
   "source": [
    "X_train_processed = prepareImages(X_train, X_train.shape[0], 'training')\n",
    "X_test_processed = prepareImages(X_test, X_test.shape[0], 'testing')\n",
    "X_train_processed /= 255\n",
    "X_test_processed /= 255\n",
    "\n",
    "# X = prepareImages(new_df['Image'], new_df['Image'].shape[0], 'training')\n",
    "# X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Image count:  (2736, 224, 224, 3)\n",
      "X_test Image count:  (114, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X_train Image count: ', X_train_processed.shape)\n",
    "print('X_test Image count: ', X_test_processed.shape)\n",
    "# print('X_test Image count: ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Setting up our y.\n",
    "new_y_array = []\n",
    "for x in y_train:\n",
    "    for y in range(6):\n",
    "        new_y_array.append(x)\n",
    "\n",
    "y_train_processed, label_encorder = prepare_labels(new_y_array)\n",
    "y_test_processed, label_encorder = prepare_labels(y_test)\n",
    "\n",
    "# y, label_encorder = prepare_labels(new_y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y train processed shape:  (2736, 10)\n",
      "Y test processed shape:  (114, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Y train processed shape: ', y_train_processed.shape)\n",
    "print('Y test processed shape: ', y_test_processed.shape)\n",
    "# print(\"Y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_processed_s, y_train_processed_s = shuffle(X_train_processed, y_train_processed, random_state=0)\n",
    "# X_test_processed_s, y_test_processed_s = shuffle(X_test_processed, y_test_processed, random_state=0)\n",
    "\n",
    "X_train_p_n, X_test_p_n, y_train_p_n, y_test_p_n = train_test_split(X_train_processed, y_train_processed, test_size=0)\n",
    "\n",
    "# print(\"X_train_p_n shape: \", X_train_p_n.shape)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and using Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg_16 = Sequential()\n",
    "# model.add()\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model_vgg_16.add(layer)\n",
    "    \n",
    "for layer in model_vgg_16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_vgg_16.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_vgg_16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg_16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/125\n",
      "2736/2736 [==============================] - 73s 27ms/step - loss: 2.2629 - acc: 0.2284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.8128 - acc: 0.3911\n",
      "Epoch 3/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.5980 - acc: 0.4920\n",
      "Epoch 4/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.4515 - acc: 0.5318\n",
      "Epoch 5/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.3529 - acc: 0.5636\n",
      "Epoch 6/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.2271 - acc: 0.6224\n",
      "Epoch 7/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.1471 - acc: 0.6597\n",
      "Epoch 8/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.0997 - acc: 0.6586\n",
      "Epoch 9/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 1.0379 - acc: 0.6959\n",
      "Epoch 10/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.9768 - acc: 0.7226\n",
      "Epoch 11/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.9508 - acc: 0.7273\n",
      "Epoch 12/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.9035 - acc: 0.7460\n",
      "Epoch 13/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.8882 - acc: 0.7449\n",
      "Epoch 14/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.8343 - acc: 0.7730\n",
      "Epoch 15/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.8165 - acc: 0.7650\n",
      "Epoch 16/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.7870 - acc: 0.7833\n",
      "Epoch 17/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.7536 - acc: 0.7993\n",
      "Epoch 18/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.7369 - acc: 0.7953\n",
      "Epoch 19/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.7220 - acc: 0.7982\n",
      "Epoch 20/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6885 - acc: 0.8176\n",
      "Epoch 21/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6734 - acc: 0.8231\n",
      "Epoch 22/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6497 - acc: 0.8297\n",
      "Epoch 23/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6321 - acc: 0.8330\n",
      "Epoch 24/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6140 - acc: 0.8447\n",
      "Epoch 25/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6085 - acc: 0.8395\n",
      "Epoch 26/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.6011 - acc: 0.8403\n",
      "Epoch 27/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5888 - acc: 0.8465\n",
      "Epoch 28/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5505 - acc: 0.8512\n",
      "Epoch 29/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5418 - acc: 0.8648\n",
      "Epoch 30/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5325 - acc: 0.8622\n",
      "Epoch 31/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5305 - acc: 0.8626\n",
      "Epoch 32/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5062 - acc: 0.8830\n",
      "Epoch 33/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.5171 - acc: 0.8681\n",
      "Epoch 34/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4886 - acc: 0.8721\n",
      "Epoch 35/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4861 - acc: 0.8739\n",
      "Epoch 36/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4650 - acc: 0.8984\n",
      "Epoch 37/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4496 - acc: 0.9002\n",
      "Epoch 38/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4535 - acc: 0.8940\n",
      "Epoch 39/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4367 - acc: 0.8969\n",
      "Epoch 40/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4250 - acc: 0.9010\n",
      "Epoch 41/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4281 - acc: 0.8933\n",
      "Epoch 42/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4299 - acc: 0.8980\n",
      "Epoch 43/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.4119 - acc: 0.9020\n",
      "Epoch 44/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3916 - acc: 0.9141\n",
      "Epoch 45/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3915 - acc: 0.9094\n",
      "Epoch 46/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3918 - acc: 0.9017\n",
      "Epoch 47/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3777 - acc: 0.9145\n",
      "Epoch 48/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3757 - acc: 0.9163\n",
      "Epoch 49/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3497 - acc: 0.9317\n",
      "Epoch 50/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3484 - acc: 0.9273\n",
      "Epoch 51/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3325 - acc: 0.9379\n",
      "Epoch 52/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3460 - acc: 0.9269\n",
      "Epoch 53/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3494 - acc: 0.9192\n",
      "Epoch 54/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3239 - acc: 0.9375\n",
      "Epoch 55/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3286 - acc: 0.9306\n",
      "Epoch 56/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3141 - acc: 0.9437\n",
      "Epoch 57/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3242 - acc: 0.9313\n",
      "Epoch 58/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3137 - acc: 0.9320\n",
      "Epoch 59/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3148 - acc: 0.9371\n",
      "Epoch 60/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2926 - acc: 0.9437\n",
      "Epoch 61/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.3003 - acc: 0.9382\n",
      "Epoch 62/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2829 - acc: 0.9492\n",
      "Epoch 63/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2979 - acc: 0.9397\n",
      "Epoch 64/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2759 - acc: 0.9518\n",
      "Epoch 65/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2806 - acc: 0.9463\n",
      "Epoch 66/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2766 - acc: 0.9441\n",
      "Epoch 67/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2738 - acc: 0.9481\n",
      "Epoch 68/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2551 - acc: 0.9539\n",
      "Epoch 69/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2627 - acc: 0.9536\n",
      "Epoch 70/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2570 - acc: 0.9554\n",
      "Epoch 71/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2479 - acc: 0.9550\n",
      "Epoch 72/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2529 - acc: 0.9543\n",
      "Epoch 73/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2433 - acc: 0.9565\n",
      "Epoch 74/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2555 - acc: 0.9550\n",
      "Epoch 75/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2327 - acc: 0.9627\n",
      "Epoch 76/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2265 - acc: 0.9653\n",
      "Epoch 77/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2416 - acc: 0.9525\n",
      "Epoch 78/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2336 - acc: 0.9576\n",
      "Epoch 79/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2227 - acc: 0.9613\n",
      "Epoch 80/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2222 - acc: 0.9609\n",
      "Epoch 81/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2135 - acc: 0.9686\n",
      "Epoch 82/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2122 - acc: 0.9682\n",
      "Epoch 83/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2208 - acc: 0.9605\n",
      "Epoch 84/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2188 - acc: 0.9616\n",
      "Epoch 85/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2009 - acc: 0.9697\n",
      "Epoch 86/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1925 - acc: 0.9755\n",
      "Epoch 87/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1961 - acc: 0.9700\n",
      "Epoch 88/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1992 - acc: 0.9656\n",
      "Epoch 89/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1965 - acc: 0.9708\n",
      "Epoch 90/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1907 - acc: 0.9737\n",
      "Epoch 91/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1890 - acc: 0.9700\n",
      "Epoch 92/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.2091 - acc: 0.9616\n",
      "Epoch 93/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1941 - acc: 0.9693\n",
      "Epoch 94/125\n",
      "2736/2736 [==============================] - 30s 11ms/step - loss: 0.1791 - acc: 0.9773\n",
      "Epoch 95/125\n",
      "2736/2736 [==============================] - 31s 11ms/step - loss: 0.1790 - acc: 0.9700\n",
      "Epoch 96/125\n",
      " 512/2736 [====>.........................] - ETA: 24s - loss: 0.1783 - acc: 0.9629"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-77bcaf5705a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_vgg_16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_p_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_p_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acons\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "history = model_vgg_16.fit(X_train_p_n, y_train_p_n, batch_size=64, epochs=125, verbose = 1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 7s 61ms/step\n",
      "The accuracy is : 0.7719298224700125\n"
     ]
    }
   ],
   "source": [
    "score_vgg16 = model_vgg_16.evaluate(X_test_processed, y_test_processed, verbose=1)\n",
    "print('The accuracy is :', score_vgg16[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg_16.save(\"vgg_16_agumented_help.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
